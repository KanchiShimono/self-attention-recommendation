# self-attention-recommendation

Keras + Tensorflow 2.0 implementation of [Self-Attentive Sequential Recommendation.](https://cseweb.ucsd.edu/~jmcauley/pdfs/icdm18.pdf)
In Proceedings of IEEE International Conference on Data Mining (ICDM'18)

## Setup

You have to set a environment variable, before you execute python scripts.

```sh
export TF_KERAS=1
```

## Related Repositories

I use some repositories to implement the self-attention-recommendation.

|Repository|Description|
|:--|:--|
|[kang205/SASRec](https://github.com/kang205/SASRec)|Official implementation|
|[CyberZHG/keras-multi-head](https://github.com/CyberZHG/keras-multi-head)|Keras implementation of multi head self attention|
|[CyberZHG/keras-pos-embd](https://github.com/CyberZHG/keras-pos-embd)|Keras implementation of position embedding|
|[CyberZHG/keras-position-wise-feed-forward](https://github.com/CyberZHG/keras-position-wise-feed-forward)|Keras implementation of point wise feed forward|
|[CyberZHG/keras-layer-normalization](https://github.com/CyberZHG/keras-layer-normalization)|Keras implementation of layer normalization|
